{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering, KMeans\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Dense, Input, Lambda\n",
    "from keras.initializers import he_normal\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.losses import mse, binary_crossentropy\n",
    "from keras.utils import plot_model\n",
    "from keras import backend as K\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ststem Parametres\n",
    "np_seed = 1998         # random seed to control the result\n",
    "\n",
    "# Control Flag, set to True or False\n",
    "# NOTICE: If the structure of the network is changed, the pretrained weight can't be employed \n",
    "use_pretrain = True   # load pretrained model, need to have vae.h5\n",
    "save_model = True     # save model\n",
    "plt_model = False     # plot model description picture\n",
    "save_predict = True   # save latent space output (with label)\n",
    "use_callback = False  # employ Earlystopping and Checkpoint during training\n",
    "\n",
    "l1_dim = 50           # encoder latent layer dimension\n",
    "l2_dim = 50           # decoder latent layer dimension\n",
    "vae_mean = 0.0        # VAE sampling normal distribution mean\n",
    "vae_std = 0.3         # VAE sampling normal distribution std deviation\n",
    "latent_dim = 3        # latent space dimension\n",
    "\n",
    "hp_epoch = 10000      # train epoch\n",
    "hp_batch_size = 512   # train batch size\n",
    "\n",
    "iot_data_dir = \"./result/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of device in the dataset, use to plot pictures later\n",
    "# **THIS CELL DOES NOT PARTICIPATE THE TRAINING OF THE SYSTEM**\n",
    "device_list = [\n",
    "    \"Amazon Echo [0]\",\"Belkin wemo motion sensor [1]\",\"Belkin Wemo switch [2]\",\n",
    "    \"Blipcare Blood Pressure meter [3]\",\"Dropcam [4]\",\"HP Printer [5]\",\n",
    "    \"iHome [6]\",\"Insteon Camera [7]\",\"Insteon Camera [8]\",\n",
    "    \"Light Bulbs LiFX Smart Bulb [9]\",\"Nest Dropcam [10]\",\"NEST Protect smoke alarm [11]\",\n",
    "    \"Netatmo weather station [12]\",\"Netatmo Welcome [13]\",\"PIX-STAR Photo-frame [14]\",\n",
    "    \"Samsung SmartCam [15]\",\"Smart Things [16]\",\"TP-Link Day Night Cloud camera [17]\",\n",
    "    \"TP-Link Smart plug [18]\",\"Triby Speaker [19]\",\"Withings Aura smart sleep sensor [20]\",\n",
    "    \"Withings Smart Baby Monitor [21]\",\"Withings Smart scale [22]\",\"Withings Baby Monitor [23]\",\n",
    "    \"Non-IoT Device [24]\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(raw_data, seed):\n",
    "    filesize = int(0.7 * len(raw_data))\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(raw_data)\n",
    "    \n",
    "    train_x = raw_data[0:filesize,1:]\n",
    "    train_y = raw_data[0:filesize,0]\n",
    "    test_x = raw_data[filesize:,1:]\n",
    "    test_y = raw_data[filesize:,0]\n",
    "    \n",
    "    return (train_x, train_y), (test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Data\n",
    "print(\"Reading IoT data...\", end='')\n",
    "raw_data = pd.read_csv(iot_data_dir + \"instance.csv\")\n",
    "print(\"Success.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Test Split\n",
    "print(\"Train Test Spliting...\")\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = train_test_split(raw_data[raw_data[\"label\"]==0].values, np_seed)\n",
    "y_train = y_train.reshape((y_train.shape[0], 1))\n",
    "y_test = y_test.reshape((y_test.shape[0],1))\n",
    "for label in range(1, 25):\n",
    "    (x_train_tmp, y_train_tmp), (x_test_tmp, y_test_tmp) = \\\n",
    "        train_test_split(raw_data[raw_data[\"label\"]==label].values, np_seed)\n",
    "    x_train = np.row_stack((x_train, x_train_tmp))\n",
    "    y_train = np.row_stack((y_train, y_train_tmp.reshape((y_train_tmp.shape[0], 1))))\n",
    "    x_test = np.row_stack((x_test, x_test_tmp))\n",
    "    y_test = np.row_stack((y_test, y_test_tmp.reshape((y_test_tmp.shape[0], 1))))\n",
    "del raw_data\n",
    "del x_train_tmp\n",
    "del y_train_tmp\n",
    "del x_test_tmp\n",
    "del y_test_tmp\n",
    "print(\"Done.\")\n",
    "\n",
    "print(\"\\nX train shape:\",x_train.shape)\n",
    "print(\"Y train shape:\",y_train.shape)\n",
    "print(\"X test shape :\",x_test.shape)\n",
    "print(\"Y test shape :\",y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Scale\n",
    "x_train = scale(x_train.astype('float64'))\n",
    "x_test = scale(x_test.astype('float64'))\n",
    "\n",
    "x_train = x_train.reshape((x_train.shape[0], -1))\n",
    "x_test = x_test.reshape((x_test.shape[0], -1))\n",
    "print(\"X train shape:\",x_train.shape)\n",
    "print(\"X test shape:\",x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(args):\n",
    "    \"\"\"Reparameterization trick by sampling fr an isotropic unit Gaussian.\n",
    "\n",
    "    # Arguments\n",
    "        args (tensor): mean and log of variance of Q(z|X)\n",
    "\n",
    "    # Returns\n",
    "        z (tensor): sampled latent vector\n",
    "    \"\"\"\n",
    "\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    # by default, random_normal has mean=0 and std=1.0\n",
    "    epsilon = K.random_normal(shape=(batch, dim), stddev=vae_std, mean=vae_mean)\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network Parameters\n",
    "original_dim = x_train.shape[1]\n",
    "input_shape = (x_train.shape[1], )\n",
    "\n",
    "epochs = hp_epoch\n",
    "batch_size = hp_batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAE model = encoder + decoder\n",
    "# build encoder model\n",
    "inputs = Input(shape=input_shape, name='encoder_input')\n",
    "x = Dense(l1_dim, activation='relu')(inputs)\n",
    "#x = Dense(l2_dim, activation='relu')(x)\n",
    "z_mean = Dense(latent_dim, name='z_mean')(x)\n",
    "z_log_var = Dense(latent_dim, name='z_log_var')(x)\n",
    "\n",
    "# use reparameterization trick to push the sampling out as input\n",
    "z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n",
    "\n",
    "# instantiate encoder model\n",
    "encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
    "encoder.summary()\n",
    "\n",
    "# build decoder model\n",
    "latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n",
    "x = Dense(l2_dim, activation='relu')(latent_inputs)\n",
    "#x = Dense(l1_dim, activation='relu')(x)\n",
    "outputs = Dense(original_dim, activation='sigmoid')(x)\n",
    "\n",
    "# instantiate decoder model\n",
    "decoder = Model(latent_inputs, outputs, name='decoder')\n",
    "decoder.summary()\n",
    "\n",
    "# instantiate VAE model\n",
    "outputs = decoder(encoder(inputs)[2])\n",
    "vae = Model(inputs, outputs, name='vae_mlp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAE loss\n",
    "reconstruction_loss = mse(inputs, outputs)\n",
    "reconstruction_loss *= original_dim\n",
    "kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
    "kl_loss = K.sum(kl_loss, axis=-1)\n",
    "kl_loss *= -0.5\n",
    "vae_loss = K.mean(reconstruction_loss + kl_loss)\n",
    "vae.add_loss(vae_loss)\n",
    "vae.compile(optimizer='adam')\n",
    "\n",
    "if plt_model:\n",
    "    plot_model(encoder, to_file='vae_mlp_encoder.png', show_shapes=True)\n",
    "    plot_model(decoder, to_file='vae_mlp_decoder.png', show_shapes=True)\n",
    "    plot_model(vae, to_file='vae_model.png', show_shapes=True)\n",
    "\n",
    "vae.summary()\n",
    "\n",
    "# callbacks\n",
    "if use_callback:\n",
    "    cb1 = ModelCheckpoint(\"./checkpoint/model.h5\",\n",
    "                         monitor='val_loss', verbose=1,save_best_only=True)\n",
    "    cb2 = EarlyStopping(monitor='loss', min_delta=0, patience=128, verbose=1, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAE training\n",
    "if use_pretrain:\n",
    "    print(\"Loading VAE weights...\",end='')\n",
    "    vae.load_weights('vae.h5')\n",
    "    print(\"Done.\")\n",
    "else:\n",
    "    if use_callback:\n",
    "        history = vae.fit(x_train,\n",
    "                    nb_epoch=hp_epoch,\n",
    "                    batch_size=hp_batch_size,\n",
    "                    callbacks=[cb1],\n",
    "                    shuffle=True, validation_data=(x_test, None))\n",
    "    else:\n",
    "        history = vae.fit(x_train,\n",
    "                    nb_epoch=hp_epoch,\n",
    "                    batch_size=hp_batch_size,\n",
    "                    shuffle=True, validation_data=(x_test, None))\n",
    "    if save_model:\n",
    "        vae.save_weights('vae.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot training history\n",
    "if not use_pretrain:\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.plot(history.history['loss'], label='train')\n",
    "    plt.plot(history.history['val_loss'], label='test')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    del history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_scatter (encoded_imgs, label, azim, elev, title=\"latent_space\", save=True):\n",
    "    ticks = np.arange(0,25)\n",
    "    fig = plt.figure(figsize=(9,6))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    p = ax.scatter(encoded_imgs[:, 0], encoded_imgs[:, 1], encoded_imgs[:, 2],\n",
    "                   c=label, cmap=plt.cm.get_cmap('jet', 25))\n",
    "    ax.view_init(azim=azim,elev=elev)\n",
    "    ax.set_title(title,fontsize=20)\n",
    "    fig.colorbar(p, ticks=ticks,fraction=0.046, pad=0.04, aspect=20)\n",
    "    fig.tight_layout()\n",
    "    if save:\n",
    "        fig.savefig(title + \".png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.reshape((y_train.shape[0]))\n",
    "y_test = y_test.reshape((y_test.shape[0]))\n",
    "\n",
    "# Test Scatter Ploting, first run should set save predict to True\n",
    "encoded_imgs, _, _ = encoder.predict(x_test, batch_size=hp_batch_size)\n",
    "if save_predict:\n",
    "    print(\"Saving test predict...\", end='')\n",
    "    y_test = y_test.reshape((y_test.shape[0],1))\n",
    "    sav = np.column_stack((y_test, encoded_imgs))\n",
    "    np.savetxt(iot_data_dir + \"test_predict.csv\", sav, delimiter=\",\")\n",
    "    print(\"  Done.\")\n",
    "\n",
    "print(\"Reading test data...\", end='')\n",
    "test_img = np.loadtxt(iot_data_dir + \"test_predict.csv\", delimiter=\",\")\n",
    "test_label = test_img[:,0]\n",
    "test_img = test_img[:,1:]\n",
    "print(\"  Success.\")\n",
    "plt_scatter(test_img,test_label, azim=48, elev=20, title=\"test_latent_space\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Scatter Ploting, first run should set save predict to True\n",
    "encoded_imgs, _, _ = encoder.predict(x_train, batch_size=hp_batch_size)\n",
    "if save_predict:\n",
    "    print(\"Saving train predict...\", end='')\n",
    "    y_train = y_train.reshape((y_train.shape[0],1))\n",
    "    sav = np.column_stack((y_train, encoded_imgs))\n",
    "    np.savetxt(iot_data_dir + \"train_predict.csv\", sav, delimiter=\",\")\n",
    "    print(\"  Done.\")\n",
    "\n",
    "print(\"Reading train data...\", end='')\n",
    "train_img = np.loadtxt(iot_data_dir + \"train_predict.csv\", delimiter=\",\")\n",
    "train_label = train_img[:,0]\n",
    "train_img = train_img[:,1:]\n",
    "print(\"  Success.\")\n",
    "plt_scatter(train_img,train_label, azim=48, elev=20, title=\"train_latent_space\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, classes, normalize=True, title=None, cmap=plt.cm.Blues):\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "    \n",
    "    device_black_list = [3, 10, 23, 8]\n",
    "    delete_list = []\n",
    "    \n",
    "    for i in range(0, y_true.shape[0]):\n",
    "        if int(y_true[i]) in device_black_list:\n",
    "            delete_list.append(i)\n",
    "    y_true = np.delete(y_true, delete_list)\n",
    "    y_pred = np.delete(y_pred, delete_list)\n",
    "    \n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "#     print(cm.shape)\n",
    "#     print(y_true.shape)\n",
    "#     print(y_pred.shape)\n",
    "    # Only use the labels that appear in the data\n",
    "    uniqueLabel = np.unique(np.row_stack((y_true,y_pred)))\n",
    "    new_class = []\n",
    "    for i in range(0, uniqueLabel.shape[0]):\n",
    "        if uniqueLabel[i] not in device_black_list:\n",
    "            new_class.append(classes[int(uniqueLabel[i])])\n",
    "    classes = new_class\n",
    "    del new_class\n",
    "    \n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    # print(cm)\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_size_inches(18, 18)\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    # ax.figure.colorbar(im, ax=ax)\n",
    "    ax.figure.colorbar(im,fraction=0.046, pad=0.04, aspect=20)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes)\n",
    "    \n",
    "    plt.xticks(fontsize=14)\n",
    "    plt.yticks(fontsize=14)\n",
    "    ax.set_ylabel('True label',fontsize=18)\n",
    "    ax.set_xlabel('Predicted label',fontsize=18)\n",
    "    ax.set_title(title, fontsize=20)\n",
    "#     ttl = ax.title\n",
    "#     ttl.set_position([.5, 1.02]) #标题距离\n",
    "    \n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "    \n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.3f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(title)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Means Model\n",
    "print(\"Training K-Means model...\",end='')\n",
    "model = KMeans(n_clusters=25, max_iter=512, n_init=128, n_jobs=16, random_state=0).fit(train_img)\n",
    "print(\"Done.\")\n",
    "\n",
    "model_label = model.labels_\n",
    "label_map = np.zeros((25,))\n",
    "\n",
    "match_cnt = np.zeros((25,25))\n",
    "for i in range(0, train_label.shape[0]):\n",
    "    match_cnt[int(train_label[i])][model_label[i]] += 1\n",
    "\n",
    "label_map = match_cnt.argmax(axis=0)\n",
    "print(\"Label map:\")\n",
    "print(label_map)\n",
    "del match_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison of true label with k-means label\n",
    "ticks = np.arange(0,25)\n",
    "fig = plt.figure(figsize=(20,7))\n",
    "\n",
    "ax1 = fig.add_subplot(121, projection='3d')\n",
    "ax1.view_init(azim=48,elev=20)\n",
    "ax1.set_title(\"True Label\",fontsize=20)\n",
    "p1 = ax1.scatter(train_img[:, 0], train_img[:, 1], train_img[:, 2],\n",
    "               c=train_label, cmap=plt.cm.get_cmap('jet', 25))\n",
    "# fig.colorbar(p1, ticks=ticks,fraction=0.046, pad=0.04, aspect=20)\n",
    "\n",
    "ax2 = fig.add_subplot(122, projection='3d')\n",
    "ax2.view_init(azim=48,elev=20)\n",
    "ax2.set_title(\"K-Means Label\",fontsize=20)\n",
    "p2 = ax2.scatter(train_img[:, 0], train_img[:, 1], train_img[:, 2],\n",
    "               c=model_label, cmap=plt.cm.get_cmap('jet', 25))\n",
    "# fig.colorbar(p2, ticks=ticks,fraction=0.046, pad=0.04, aspect=20)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "del ticks\n",
    "del ax1\n",
    "del p1\n",
    "del ax2\n",
    "del p2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total Accuracy Calculation\n",
    "\"\"\"\n",
    "NOTICE: SINCE WE DID NOT CONTROL THE INITIALIZATION OF THE NEURAL NETWORK\n",
    "        IT IS POSSIBLE FOR THE ACCURACY TO BE DIFFERENT FROM THE PAPER\n",
    "        BUT THE RESULT SHOULD VIBRATE NEAR 85%\n",
    "\"\"\"\n",
    "\n",
    "pred = model.predict(test_img)\n",
    "\n",
    "acc = 0\n",
    "acc_cls = np.zeros((25,2))\n",
    "for i in range(0, test_label.shape[0]):\n",
    "    pred[i] = label_map[pred[i]]\n",
    "    acc_cls[int(test_label[i])][0] += 1\n",
    "    if test_label[i] == pred[i]:\n",
    "        acc += 1\n",
    "        acc_cls[int(test_label[i])][1] += 1\n",
    "acc = acc / test_label.shape[0]\n",
    "print(\"Total acc: %.3f\"%(acc*100) + \" %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(test_label, pred, device_list, title=\"Unsupervised Classification System Confusion Matrix\", cmap=plt.cm.OrRd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(test_label, pred, device_list, title=\"Unsupervised Classification System Confusion Matrix (unNorm)\", cmap=plt.cm.OrRd, normalize=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
